{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUohQld_ZA5C"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If running on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXub2MWNaqMX",
    "outputId": "11da377b-bed2-4139-f21d-3bfe4c006169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fU_DI1DuZA5S"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T03:10:47.006507Z",
     "start_time": "2021-04-20T03:10:34.084473Z"
    },
    "id": "vP3p2Xr2ZA5V"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import unicodedata\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.layers import LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T03:11:06.660283Z",
     "start_time": "2021-04-20T03:11:03.854235Z"
    },
    "id": "o79z4-0-ZA5W"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T09:55:58.688074Z",
     "start_time": "2021-04-18T09:55:58.679056Z"
    }
   },
   "outputs": [],
   "source": [
    "re_url = re.compile(r'(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
    "re_email = re.compile('(?:[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b09Rku3CZA5l"
   },
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:57:51.172528Z",
     "start_time": "2021-04-18T10:57:51.168530Z"
    },
    "id": "TiHqBOmUZA5n"
   },
   "outputs": [],
   "source": [
    "categories = ['soc.religion.christian', 'sci.space', 'sci.electronics', 'talk.religion.misc', 'rec.motorcycles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leswN5W4ZA5o"
   },
   "outputs": [],
   "source": [
    "newsgroups_train_data = fetch_20newsgroups(data_home='20_Newsgroup_Data/',\n",
    "                                           subset='train', categories=categories)\n",
    "newsgroups_test_data = fetch_20newsgroups(data_home='20_Newsgroup_Data/',\n",
    "                                          subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSzjuPyEZA5p"
   },
   "source": [
    "**Train Dataset DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW--y5Q-ZA5r"
   },
   "outputs": [],
   "source": [
    "train_data_newsgroup_df = pd.DataFrame({\"Data\": newsgroups_train_data['data'], \"Target\": newsgroups_train_data['target']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1bUw5UeZA5s"
   },
   "outputs": [],
   "source": [
    "train_data_newsgroup_df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVtK0WpBZA5u"
   },
   "source": [
    "**Test Dataset DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lvcObbhZA5v"
   },
   "outputs": [],
   "source": [
    "test_data_newsgroup_df = pd.DataFrame({\"Data\": newsgroups_test_data['data'], \"Target\": newsgroups_test_data['target']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6axSKSy5ZA5v"
   },
   "outputs": [],
   "source": [
    "test_data_newsgroup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5ByX8oaZA5w"
   },
   "outputs": [],
   "source": [
    "# Codes to save the train and test dataframes.\n",
    "\n",
    "train_data_newsgroup_df.to_csv('train_data_newsgroup_df.csv', sep = '\\t')\n",
    "test_data_newsgroup_df.to_csv('test_data_newsgroup_df.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the train and test dataframes which are already saved to avoid interacting with the sklearn.dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2rIwZ9iZA5x"
   },
   "outputs": [],
   "source": [
    "# Codes for loading train and test dataframes.\n",
    "\n",
    "train_data_newsgroup_df_loaded = pd.read_csv('train_data_newsgroup_df.csv', sep = '\\t')\n",
    "train_data_newsgroup_df = train_data_newsgroup_df_loaded[['Data', 'Target']]\n",
    "\n",
    "test_data_newsgroup_df_loaded = pd.read_csv('test_data_newsgroup_df.csv', sep = '\\t')\n",
    "test_data_newsgroup_df = test_data_newsgroup_df_loaded[['Data', 'Target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUep50pTZA5y"
   },
   "source": [
    "# Utility Functions for Preprocessing the 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeP79v0SZA5z"
   },
   "outputs": [],
   "source": [
    "def clean_header(text):\n",
    "    text = re.sub(r'(From:\\s+[^\\n]+\\n)', '', text)\n",
    "    text = re.sub(r'(Subject:[^\\n]+\\n)', '', text)\n",
    "    text = re.sub(r'(([\\sA-Za-z0-9\\-]+)?[A|a]rchive-name:[^\\n]+\\n)', '', text)\n",
    "    text = re.sub(r'(Last-modified:[^\\n]+\\n)', '', text)\n",
    "    text = re.sub(r'(Version:[^\\n]+\\n)', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_text(text):        \n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(re_url, '', text)\n",
    "    text = re.sub(re_email, '', text)\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = re.sub(r'(\\d+)', ' ', text)\n",
    "    text = re.sub(r'(\\s+)', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = clean_header(text)\n",
    "    text = clean_text(text)\n",
    "    return text\n",
    "\n",
    "def add_start_end_tokens(string):\n",
    "    return '<start> ' + string + ' <end>'\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqwjxG2AZA50"
   },
   "source": [
    "# Preprocess the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNPfdwXTZA51"
   },
   "outputs": [],
   "source": [
    "train_data_newsgroup_df['Data Cleaned'] = train_data_newsgroup_df['Data'].apply(preprocess)\n",
    "train_data_newsgroup_df['Data Cleaned'] = train_data_newsgroup_df['Data Cleaned'].str.split().apply(lambda x: ' '.join([word for word in x if word not in stop_words]))\n",
    "train_data_newsgroup_df['Data Cleaned'] = train_data_newsgroup_df['Data Cleaned'].apply(add_start_end_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlTGDZtiZA52"
   },
   "outputs": [],
   "source": [
    "train_data_newsgroup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uxp8GOI9ZA52"
   },
   "outputs": [],
   "source": [
    "test_data_newsgroup_df['Data Cleaned'] = test_data_newsgroup_df['Data'].apply(preprocess)\n",
    "test_data_newsgroup_df['Data Cleaned'] = test_data_newsgroup_df['Data Cleaned'].str.split().apply(lambda x: ' '.join([word for word in x if word not in stop_words]))\n",
    "test_data_newsgroup_df['Data Cleaned'] = test_data_newsgroup_df['Data Cleaned'].apply(add_start_end_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liGcrtwQZA53"
   },
   "outputs": [],
   "source": [
    "test_data_newsgroup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes for loading the cleaned train and test dataframes to save time IF running on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T08:56:10.208441Z",
     "start_time": "2021-04-18T08:56:09.940691Z"
    },
    "id": "1nSgWEchZA53"
   },
   "outputs": [],
   "source": [
    "# Codes for loading the CLEANED train and test dataframes.\n",
    "\n",
    "train_data_newsgroup_df_loaded = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DE-P05 NLP MODEL/train_data_newsgroup_df.csv', sep = '\\t')\n",
    "train_data_newsgroup_df = train_data_newsgroup_df_loaded[['Data', 'Target', 'Data Cleaned']]\n",
    "\n",
    "test_data_newsgroup_df_loaded = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DE-P05 NLP MODEL/test_data_newsgroup_df.csv', sep = '\\t')\n",
    "test_data_newsgroup_df = test_data_newsgroup_df_loaded[['Data', 'Target', 'Data Cleaned']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes for loading the cleaned train and test dataframes to save time IF running on Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T09:56:14.475901Z",
     "start_time": "2021-04-18T09:56:13.913785Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_newsgroup_df_loaded = pd.read_csv('train_data_newsgroup_df.csv', sep = '\\t')\n",
    "train_data_newsgroup_df = train_data_newsgroup_df_loaded[['Data', 'Target', 'Data Cleaned']]\n",
    "\n",
    "test_data_newsgroup_df_loaded = pd.read_csv('test_data_newsgroup_df.csv', sep = '\\t')\n",
    "test_data_newsgroup_df = test_data_newsgroup_df_loaded[['Data', 'Target', 'Data Cleaned']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkEsTq5EZA55"
   },
   "source": [
    "**Start setting up the datasets, `X_train`, `y_train`, `X_test` and `y_test`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T09:56:17.707159Z",
     "start_time": "2021-04-18T09:56:17.703169Z"
    },
    "id": "Kj-izVhAZA56"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train_data_newsgroup_df['Data Cleaned'].to_numpy(), train_data_newsgroup_df['Target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T09:56:18.195186Z",
     "start_time": "2021-04-18T09:56:18.191222Z"
    },
    "id": "iAPBJfTrZA57"
   },
   "outputs": [],
   "source": [
    "X_test, y_test = test_data_newsgroup_df['Data Cleaned'].to_numpy(), test_data_newsgroup_df['Target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T09:56:18.664478Z",
     "start_time": "2021-04-18T09:56:18.661438Z"
    },
    "id": "9ULvLlyBZA57"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower = False,filters= '', oov_token = '<UKW>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T09:56:19.685500Z",
     "start_time": "2021-04-18T09:56:19.239645Z"
    },
    "id": "QePSLX8XZA58"
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:55:17.502596Z",
     "start_time": "2021-04-18T10:55:17.496613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  673, 1723,    4],\n",
       "       [   0,    0,    0, ..., 1027,  121,    4],\n",
       "       [   0,    0,    0, ...,   43,  178,    4],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 5319,   64,    4],\n",
       "       [   0,    0,    0, ..., 2227, 5655,    4],\n",
       "       [   0,    0,    0, ..., 3281, 9259,    4]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the tokenizer's configuration as a .json file, so as to reload it when preprocessing incoming tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:35:47.881058Z",
     "start_time": "2021-04-18T10:35:47.661659Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_config_string = tokenizer.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:37:04.370106Z",
     "start_time": "2021-04-18T10:37:04.319243Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_config_filename = os.path.join('.','tokenizer_config.json')\n",
    "\n",
    "with open(tokenizer_config_filename, 'w') as file:\n",
    "    json.dump(tokenizer_config_string, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the size of the vocabulary in the 20 Newsgroup dataset and the maximum length of all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:46:49.141509Z",
     "start_time": "2021-04-18T10:46:49.136520Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59su9kKiZA59",
    "outputId": "3d335ebf-bbd2-4a0d-dec9-c99251ec34c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34073"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:46:49.736133Z",
     "start_time": "2021-04-18T10:46:49.729152Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pp_J2wGZA6A",
    "outputId": "fb799e66-8fbe-4cf0-b332-f3473d4d4d25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6279, 2649)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length = -1\n",
    "\n",
    "for idxx, i in enumerate(X_train):\n",
    "    if len(i) > max_sentence_length:\n",
    "        max_sentence_length = len(i)\n",
    "        idx = idxx\n",
    "        \n",
    "max_sentence_length, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store `vocab_size` and `max_sentence_length` in a .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:46:52.089441Z",
     "start_time": "2021-04-18T10:46:52.085450Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn_model_params = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"max_sentence_length\": max_sentence_length\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:47:28.097332Z",
     "start_time": "2021-04-18T10:47:28.092279Z"
    }
   },
   "outputs": [],
   "source": [
    "model_params_filename = os.path.join('.','model_params.json')\n",
    "\n",
    "with open(model_params_filename, 'w') as file:\n",
    "    json.dump(rnn_model_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad the sentences to its maximum length with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:52:03.950741Z",
     "start_time": "2021-04-18T10:52:03.872948Z"
    },
    "id": "Ghh7tvsfZA6B"
   },
   "outputs": [],
   "source": [
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_sentence_length, padding='pre', truncating = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T08:57:56.797301Z",
     "start_time": "2021-04-18T08:57:56.746395Z"
    },
    "id": "6YFz0C3eZA6B"
   },
   "outputs": [],
   "source": [
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_sentence_length, padding='pre', truncating = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:52:51.811004Z",
     "start_time": "2021-04-18T10:52:51.805020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     3,\n",
       "           5,   937,   172,   484,     2,     9,  1026,  5486,     8,\n",
       "        1964,   472,  1756,  4322,    42,   183,  4088,   101,  3400,\n",
       "        1048,  4573,  3561,  3144, 14279,  9733,  1048, 14280,  5905,\n",
       "       11551,   440,  3259,  2454,    16,  3260,  3562,  3145, 11552,\n",
       "        3145,    32,  1049,  1347,  1280,  4574,   937,  1027,   121,\n",
       "           4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1,-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a tensorflow.data.Dataset object out of the numpy arrays of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T08:57:58.034129Z",
     "start_time": "2021-04-18T08:57:57.186114Z"
    },
    "id": "RBA2-T5HZA6C"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T08:57:58.603285Z",
     "start_time": "2021-04-18T08:57:58.592313Z"
    },
    "id": "lJMHSq_qZA6C"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = train_dataset.batch(16)\n",
    "test_dataset = test_dataset.batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T08:58:00.485372Z",
     "start_time": "2021-04-18T08:57:59.236180Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFczkTCdZA6D",
    "outputId": "4926c84f-75aa-41d8-c3a9-fa7b72d0d465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 256)         8722944   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 256)               394752    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 9,118,981\n",
      "Trainable params: 9,118,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(vocab_size):\n",
    "    \"\"\"\n",
    "    This function takes a vocabulary size and batch size, and builds and returns a \n",
    "    Sequential model according to the above specification.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = 256, mask_zero = True),\n",
    "        tf.keras.layers.GRU(units = 256),\n",
    "        tf.keras.layers.Dense(5)\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "rnn_model = get_model(vocab_size = vocab_size+1)\n",
    "\n",
    "rnn_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the RNN Model if running on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_flag = False # A flag indicating if one should proceed with training, False => DO NOT PROCEED with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CmIPynGBkS8H"
   },
   "outputs": [],
   "source": [
    "if load_model_flag: # This prevents the training from running accidentally.\n",
    "    rnn_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/DE-P05 NLP MODEL/rnn_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the RNN Model if running on Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:39:56.188604Z",
     "start_time": "2021-04-18T10:39:56.185588Z"
    }
   },
   "outputs": [],
   "source": [
    "load_model_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:40:00.091445Z",
     "start_time": "2021-04-18T10:39:56.972027Z"
    }
   },
   "outputs": [],
   "source": [
    "if load_model_flag:\n",
    "    rnn_model = tf.keras.models.load_model('models/rnn_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train or re-train the RNN model if need be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_flag = False # A flag indicating if one should proceed with training, False => DO NOT PROCEED with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AsbNAf-z-t-V",
    "outputId": "0d0af84d-55da-4b08-c227-5d4150adf91f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "173/173 [==============================] - 2441s 14s/step - loss: 0.0741 - sparse_categorical_accuracy: 0.9775\n",
      "Epoch 2/2\n",
      "173/173 [==============================] - 2417s 14s/step - loss: 0.0238 - sparse_categorical_accuracy: 0.9949\n"
     ]
    }
   ],
   "source": [
    "if train_model_flag: # This prevents the training from running accidentally.\n",
    "    history = rnn_model.fit(train_dataset, epochs = 2, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last training done on 17 April 2021 achieved such a training accuracy.\n",
    "\n",
    "```\n",
    "Epoch 1/2\n",
    "173/173 [==============================] - 2441s 14s/step - loss: 0.0741 - sparse_categorical_accuracy: 0.9775\n",
    "Epoch 2/2\n",
    "173/173 [==============================] - 2417s 14s/step - loss: 0.0238 - sparse_categorical_accuracy: 0.9949\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE the RNN Model if running on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QMX-cq5KabI-"
   },
   "outputs": [],
   "source": [
    "if save_flag:\n",
    "    rnn_model.save('/content/drive/MyDrive/Colab Notebooks/DE-P05 NLP MODEL/rnn_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE the RNN Model if running on Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_flag:\n",
    "    rnn_model.save('models/rnn_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the RNN Model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T03:24:47.878344Z",
     "start_time": "2021-04-20T03:24:47.861352Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6ib-XRl3fR5",
    "outputId": "609c1a4b-bbb8-43e1-c892-3f4c1345c4ad"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bf4d9239856b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "rnn_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK52v-zFZA6p"
   },
   "source": [
    "On 17 April 2021, model was evaluated, results as follow:\n",
    "\n",
    "```\n",
    "115/115 [==============================] - 109s 939ms/step - loss: 0.6161 - sparse_categorical_accuracy: 0.8244\n",
    "Loss, Accuracy = [0.6161214709281921, 0.8244274854660034]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply model on Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tweets dataframe first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:11:10.477412Z",
     "start_time": "2021-04-18T10:11:10.121939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_df=pd.read_csv('train_tweets.csv')\n",
    "train_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a generator to yield a random tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:10:05.241329Z",
     "start_time": "2021-04-18T11:10:05.235338Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator_random_tweet(train_tweets_df):\n",
    "    random_int = np.random.randint(len(train_tweets_df))\n",
    "    random_single_tweet = train_tweets_df['tweet'][random_int]\n",
    "    yield random_single_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:12:50.396417Z",
     "start_time": "2021-04-18T10:12:50.385448Z"
    }
   },
   "outputs": [],
   "source": [
    "random_int = np.random.randint(len(train_tweets_df))\n",
    "\n",
    "random_single_tweet = train_tweets_df['tweet'][random_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:26:50.431465Z",
     "start_time": "2021-04-18T10:26:50.426498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('@user is this for real? #waspi 50sborn in abject povey but we are all in it together! @user  ',\n",
       " 21960)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_single_tweet, random_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for preprocessing the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:24:03.674936Z",
     "start_time": "2021-04-18T10:24:03.667954Z"
    }
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "### strip_linkes and strip_all_entities from\n",
    "### https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression\n",
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords_and_lemmatize(text):\n",
    "    stopword = stopwords.words('english')\n",
    "    text_splitted = text.split()    \n",
    "    text = \" \".join([wnl.lemmatize(word) for word in text_splitted if word not in stopword])\n",
    "    return text\n",
    "\n",
    "def preprocess_single_tweet(single_tweet):\n",
    "    \"\"\"single_tweet is a string, a tweet, output is another string which is processed.\"\"\"\n",
    "    \n",
    "    single_tweet = remove_stopwords_and_lemmatize(strip_links(single_tweet))\n",
    "    single_tweet = (lambda single_twt: re.sub(r'[^a-zA-Z]', ' ', single_twt))(single_tweet)\n",
    "    single_tweet = (lambda x: re.sub('  ', ' ', x))(single_tweet)\n",
    "    \n",
    "    return single_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:53:14.028578Z",
     "start_time": "2021-04-18T10:53:14.015619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before processed: @user is this for real? #waspi 50sborn in abject povey but we are all in it together! @user  \n",
      "After processed:  user real  waspi  sborn abject povey together  user\n"
     ]
    }
   ],
   "source": [
    "example_tweet = preprocess_single_tweet(random_single_tweet)\n",
    "print(f\"Before processed: {random_single_tweet}\\nAfter processed: {example_tweet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tokenizer's configuration and tokenize the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:28:28.718896Z",
     "start_time": "2021-04-18T10:28:28.715971Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_config_filename = os.path.join('.','tokenizer_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:37:31.073440Z",
     "start_time": "2021-04-18T10:37:30.906848Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(tokenizer_config_filename) as file:\n",
    "    # Load its content and make a new json string\n",
    "    tokenizer_config_string = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:37:41.907550Z",
     "start_time": "2021-04-18T10:37:41.735012Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_config_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:56:57.324443Z",
     "start_time": "2021-04-18T10:56:57.320453Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_tweet = tokenizer.texts_to_sequences([example_tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:56:58.694958Z",
     "start_time": "2021-04-18T10:56:58.688976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1499, 120, 1, 1, 1, 1, 490, 1499]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the model params and the model, then pad the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_filename = os.path.join('.','model_params.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:49:06.799759Z",
     "start_time": "2021-04-18T10:49:06.795751Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(model_params_filename) as file:\n",
    "    # Load its content and make a new json string\n",
    "    model_params = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:49:35.323021Z",
     "start_time": "2021-04-18T10:49:35.320029Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = model_params['vocab_size']\n",
    "max_sentence_length = model_params['max_sentence_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:57:12.251350Z",
     "start_time": "2021-04-18T10:57:12.245366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    1,  490, 1499]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet = tf.keras.preprocessing.sequence.pad_sequences(tokenized_tweet, maxlen=max_sentence_length, padding='pre', truncating = 'pre')\n",
    "tokenized_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the category of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:57:17.025764Z",
     "start_time": "2021-04-18T10:57:16.439375Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = rnn_model.predict(tokenized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:02:11.447923Z",
     "start_time": "2021-04-18T11:02:11.443932Z"
    }
   },
   "outputs": [],
   "source": [
    "category = categories[np.squeeze(np.argmax(prediction, axis = -1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:02:11.993005Z",
     "start_time": "2021-04-18T11:02:11.989971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap the entire process of prediction on a tweet emitted from Kakfa's Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T03:11:15.569663Z",
     "start_time": "2021-04-20T03:11:10.669186Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator_random_tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5988aa05d8e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Get to be predicted tweets from Kafka, then predict them one by one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mrandom_single_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_random_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tweets_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Simulating Kafka emitting the tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mexample_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_single_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_single_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mtokenized_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexample_tweet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generator_random_tweet' is not defined"
     ]
    }
   ],
   "source": [
    "load_model_flag = True\n",
    "\n",
    "### Get the tokenizer's and model's parameters ###\n",
    "tokenizer_config_filename = os.path.join('.','tokenizer_config.json')\n",
    "model_params_filename = os.path.join('.','model_params.json')\n",
    "\n",
    "with open(tokenizer_config_filename) as file:\n",
    "    # Load its content and make a new json string\n",
    "    tokenizer_config_string = json.load(file)\n",
    "\n",
    "### Get the tokenizer ###\n",
    "tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_config_string)\n",
    "\n",
    "with open(model_params_filename) as file:\n",
    "    # Load its content and make a new json string\n",
    "    model_params = json.load(file)\n",
    "    \n",
    "vocab_size = model_params['vocab_size']\n",
    "max_sentence_length = model_params['max_sentence_length']\n",
    "\n",
    "if load_model_flag:\n",
    "    rnn_model = tf.keras.models.load_model('models/rnn_model_weights.h5')\n",
    "    \n",
    "# Get to be predicted tweets from Kafka, then predict them one by one\n",
    "for _ in range(50):\n",
    "    random_single_tweet = next(generator_random_tweet(train_tweets_df)) # Simulating Kafka emitting the tweets\n",
    "    example_tweet = preprocess_single_tweet(random_single_tweet)\n",
    "    tokenized_tweet = tokenizer.texts_to_sequences([example_tweet])\n",
    "    tokenized_tweet = tf.keras.preprocessing.sequence.pad_sequences(tokenized_tweet, maxlen=max_sentence_length, padding='pre', truncating = 'pre')\n",
    "    prediction = rnn_model.predict(tokenized_tweet)\n",
    "    category = categories[np.squeeze(np.argmax(prediction, axis = -1))]\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    The original tweet is: {random_single_tweet}.\\n\n",
    "    Predicted Category is: {category}.\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model Training for NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
