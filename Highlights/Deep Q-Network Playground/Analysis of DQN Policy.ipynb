{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of DQN Policy\n",
    "\n",
    "Author: Soon Siang <br>\n",
    "Date: 3 April 2021\n",
    "\n",
    "![Image](./images_analysis/Github_Repository_DQN_PG.png)\n",
    "\n",
    "# Objective\n",
    "\n",
    "This notebook will analyze how different mechanisms of choosing an action will result in differences in the distribution of scores attained by the agent playing cartpole, a game where the agent objective's is to balance an inverted pendulum on a cart.\n",
    "\n",
    "Three simple policies are analyzed:\n",
    "\n",
    "1) Epsilon-greedy policy.<br>\n",
    "2) Epsilon-greedy policy with Boltzmann action.<br>\n",
    "3) Boltzmann action.<br>\n",
    "\n",
    "## Epsilon-greedy policy\n",
    "\n",
    "![Image](./images_analysis/Screenshot-2020-03-18-at-8.03.38-PM.png)\n",
    "<center><b>Epsilon-greedy</b></center>\n",
    "Source: https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/\n",
    "\n",
    "As above, epsilon-greedy policy implies that when the agent is deciding which action to take, with probability epsilon, it will take the Q-value maximizing action, and with probability 1-epsilon, it will take a random action from the action space. The codes below show the implementation of such policy:\n",
    "\n",
    "```python\n",
    "    def eps_greedy(self, state):\n",
    "\n",
    "        rnd = random.random()\n",
    "        if rnd < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        return self.get_greedy_action(state)\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "\n",
    "        Q_values = self.net.model.predict(state)[0]\n",
    "        greedy_action = np.argmax(Q_values)\n",
    "        \n",
    "        return greedy_action\n",
    "```\n",
    "\n",
    "\n",
    "## Epsilon-greedy policy with Boltzmann action\n",
    "\n",
    "Similar to epsilon-greedy policy except that with 1-epsilon probability, the agent will choose action in the action space based on the Boltzmann distribution (termed as `Boltzmann action`), given below:\n",
    "\n",
    "![Image](./images_analysis/1_176NhUn1CcPHofNdVmGYhQ.png)\n",
    "<center><b>Boltzmann's Softmax</b></center>\n",
    "Source: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf\n",
    "\n",
    "The implementation is as follows:\n",
    "\n",
    "```python\n",
    "    def eps_greedy_boltzmann(self, state):\n",
    "\n",
    "        rnd = random.random()\n",
    "        if rnd < self.epsilon:\n",
    "            return self.boltzmann_action_with_temperature(state)\n",
    "        return self.get_greedy_action(state)\n",
    "    \n",
    "    def boltzmann_action_with_temperature(self, state):\n",
    "\n",
    "        Q_values = self.net.model.predict(state)[0] / self.epsilon\n",
    "        log_probs = Q_values - logsumexp(Q_values)\n",
    "        [action] = np.random.choice(self.action_size, size = 1, p = np.exp(log_probs))\n",
    "        \n",
    "        return action\n",
    "    ```\n",
    "    \n",
    "## Boltzmann action\n",
    "\n",
    "Lastly, the policy that the agent will take is only the `Boltzmann action`, given in the above equation and with its implementation shown above as the function `boltzmann_action_with_temperature()`.\n",
    "\n",
    "## Basis for Comparison\n",
    "\n",
    "These three policies, in a sense, be placed on a continuum between putting different weights from more reliance on the model given by DQN (Boltzmann action) to more freedom for the agent to choose an action base on a uniform prior (i.e. no additional information given to the model when it is free to choose any action from the action space in the epsilon-greedy policy).\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "The agent is trained for 200 episodes, using the 'Cartpole' environment provided by [OpenAI Gym](https://gym.openai.com/).\n",
    "\n",
    "The basis of comparison for the effectiveness of different policies is the distribution of the scores attained by the trained-agent for 200 episodes of playing the game. When playing the game, i.e. balancing the cartpole, the agent always choose the greedy action given by the model.<br>\n",
    "\n",
    "The maximum score that an agent is capable of attaining before the episode is terminated artificially is 199.<br>\n",
    "\n",
    "Therefore, one can roughly quantify the agent's effectiveness at playing the cartpole game by looking at the percentage of games in which the agent attains the maximum score of 199.<br>\n",
    "\n",
    "## Related Files\n",
    "\n",
    "The files relating to this experiment-playground are:\n",
    "\n",
    "1) main.py: containing train() and play() functions to allow the agent to train and play in the environment respectively. <br>\n",
    "2) agent.py: the implementation of the Agent class.<br>\n",
    "3) model.py: the implementation of the Model class.<br>\n",
    "4) memory.py: the implementation of the ExperienceReplay class.<br>\n",
    "\n",
    "The implementation of the ExperienceReply class is taken from [here](https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c).\n",
    "\n",
    "The implementation of the get_training_data() method within the Agent class is taken from [here](https://github.com/cwchong/coursework/blob/main/210330_pract9/cartpoleDQN-start/agent.py).\n",
    "\n",
    "## Model\n",
    "\n",
    "The model is a simple neural network with a dropout layer as given below:\n",
    "\n",
    "```python\n",
    "    self.model = Sequential([\n",
    "        Dense(24, activation='relu', input_shape=(state_size, )),\n",
    "        Dropout(0.2),\n",
    "        Dense(24, activation='relu'),\n",
    "        Dense(action_size, activation='linear')\n",
    "    ])\n",
    "```\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-greedy policy\n",
    "\n",
    "![Image](./images_analysis/epsg_episode_200_histo_play_scores.png)\n",
    "<center><b>Epilson-greedy</b></center>\n",
    "\n",
    "As seen from the histogram above, the agent trained with such policy is able to attain the maximum score around 100% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilson-greedy with Boltzmann action\n",
    "\n",
    "![Image](./images_analysis/BM_test_episode_200_histo_scores.png)\n",
    "<center><b>Epilson-greedy with Boltzmann action</b></center>\n",
    "\n",
    "As seen from the histogram above, the agent trained with such policy is able to attain the maximum score around 70% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boltzmann action\n",
    "![Image](./images_analysis/BM_onlyTest_episode_200_histo_play_scores.png)\n",
    "<center><b>Boltzmann action</b></center>\n",
    "\n",
    "As seen from the histogram above, the agent trained with such policy is able to attain the maximum score around 17.5% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "|                                        \t| Epsilon-greedy \t| Epsilon-greedy with Boltzmann action \t| Pure Boltzmann action \t|\n",
    "|:--------------------------------------:\t|:--------------:\t|:------------------------------------:\t|:---------------------:\t|\n",
    "| Percentage of maximum attainable score \t|      100%      \t|                  70%                 \t|         17.5%         \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that epsilon-greedy policy beats the other variants.\n",
    "\n",
    "## Bias in exploration; the spirit of exploration\n",
    "\n",
    "The reason for the worse-performance of the other two variants of policy may be due to a bias in exploration. These policies place a bias by favoring the action believed by the neural network to be Q values maximizing. Consequently, exploration of the state space is more limited than exploring with a uniform prior on the actions space. This can also be said to be going against the spirit of pure exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "By conducting this simple experiment, it highlights the importance of exploitation-versus-exploration trade-off and how when choosing to explore, the agent ought to be exploring with a uniform prior over the action space. Doing this will lead a greater region of the Q-space being included into the Experience Replay buffer such that the model will better approximate the actual Q-landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
