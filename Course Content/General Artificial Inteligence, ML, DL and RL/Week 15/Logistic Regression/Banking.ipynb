{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression** is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**    \n",
    "The dataset comes from the UCI Machine Learning repository, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict whether the client will subscribe (1/0) to a term deposit (variable y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "#plt.rc(\"font\", size=14)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "#sns.set(style=\"white\")\n",
    "#sns.set(style=\"whitegrid\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides the bank customers’ information. It includes 41,188 records and 21 fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"banking.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input variables**    \n",
    "1. age (numeric)\n",
    "2. job : type of job (categorical: “admin”, “blue-collar”, “entrepreneur”, “housemaid”, “management”, “retired”, “self-employed”, “services”, “student”, “technician”, “unemployed”, “unknown”)\n",
    "3. marital : marital status (categorical: “divorced”, “married”, “single”, “unknown”)\n",
    "4. education (categorical: “basic.4y”, “basic.6y”, “basic.9y”, “high.school”, “illiterate”, “professional.course”, “university.degree”, “unknown”)\n",
    "5. default: has credit in default? (categorical: “no”, “yes”, “unknown”)\n",
    "6. housing: has housing loan? (categorical: “no”, “yes”, “unknown”)\n",
    "7. loan: has personal loan? (categorical: “no”, “yes”, “unknown”)\n",
    "8. contact: contact communication type (categorical: “cellular”, “telephone”)\n",
    "9. month: last contact month of year (categorical: “jan”, “feb”, “mar”, …, “nov”, “dec”)\n",
    "10. day_of_week: last contact day of the week (categorical: “mon”, “tue”, “wed”, “thu”, “fri”)\n",
    "11. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y=’no’). The duration is not known before a call is performed, also, after the end of the call, y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model\n",
    "12. : number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14. previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15. poutcome: outcome of the previous marketing campaign (categorical: “failure”, “nonexistent”, “success”)\n",
    "16. emp.var.rate: employment variation rate — (numeric)\n",
    "17. cons.price.idx: consumer price index — (numeric)\n",
    "18. cons.conf.idx: consumer confidence index — (numeric)\n",
    "19. euribor3m: euribor 3 month rate — (numeric)\n",
    "20. nr.employed: number of employees — (numeric)\n",
    "\n",
    "\n",
    "**Predict variable (desired target):**  \n",
    "y — has the client subscribed a term deposit? (binary: “1”, means “Yes”, “0” means “No”)  \n",
    "\n",
    "\n",
    "The education column of the dataset has many categories and we need to reduce the categories for a better modelling. The education column has the following categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['education'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us group “basic.4y”, “basic.9y” and “basic.6y” together and call them “basic”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['education']=np.where(data['education'] =='basic.9y', 'Basic', data['education'])\n",
    "data['education']=np.where(data['education'] =='basic.6y', 'Basic', data['education'])\n",
    "data['education']=np.where(data['education'] =='basic.4y', 'Basic', data['education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['education'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data exploration**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='y', data=data, palette='hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_no_sub = len(data[data['y']==0])\n",
    "count_sub = len(data[data['y']==1])\n",
    "pct_of_no_sub = count_no_sub/(count_no_sub+count_sub)\n",
    "print(\"percentage of no subscription is\", pct_of_no_sub*100)\n",
    "pct_of_sub = count_sub/(count_no_sub+count_sub)\n",
    "print(\"percentage of subscription\", pct_of_sub*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classes are imbalanced, and the ratio of no-subscription to subscription instances is 89:11. Before we go ahead to balance the classes, let’s do some more exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('y').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**  \n",
    "- The average age of customers who bought the term deposit is higher than that of the customers who didn’t.  \n",
    "- The pdays (days since the customer was last contacted) is understandably lower for the customers who bought it. The lower the pdays, the better the memory of the last call and hence the better chances of a sale.  \n",
    "- Surprisingly, campaigns (number of contacts or calls made during the current campaign) are lower for customers who bought the term deposit.  \n",
    "\n",
    "\n",
    "We can calculate categorical means for other categorical variables such as education and marital status to get a more detailed sense of our data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('job').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('marital').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('education').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizations**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.crosstab(data.job,data.y).plot(kind='bar')\n",
    "plt.title('Purchase Frequency for Job Title')\n",
    "plt.xlabel('Job')\n",
    "plt.ylabel('Frequency of Purchase')\n",
    "plt.savefig('purchase_fre_job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of purchase of the deposit depends a great deal on the job title. Thus, the job title can be a good predictor of the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.crosstab(data.marital,data.y)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Stacked Bar Chart of Marital Status vs Purchase')\n",
    "plt.xlabel('Marital Status')\n",
    "plt.ylabel('Proportion of Customers')\n",
    "plt.savefig('mariral_vs_pur_stack')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marital status does not seem a strong predictor for the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.crosstab(data.education,data.y)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Stacked Bar Chart of Education vs Purchase')\n",
    "plt.xlabel('Education')\n",
    "plt.ylabel('Proportion of Customers')\n",
    "plt.savefig('edu_vs_pur_stack')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Education seems a good predictor of the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.day_of_week,data.y).plot(kind='bar')\n",
    "plt.title('Purchase Frequency for Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Frequency of Purchase')\n",
    "plt.savefig('pur_dayofweek_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day of week may not be a good predictor of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.month,data.y).plot(kind='bar')\n",
    "plt.title('Purchase Frequency for Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Frequency of Purchase')\n",
    "plt.savefig('pur_fre_month_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Month might be a good predictor of the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.age.hist()\n",
    "plt.title('Histogram of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('hist_age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the customers of the bank in this dataset are in the age range of 30–40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.poutcome,data.y).plot(kind='bar')\n",
    "plt.title('Purchase Frequency for Poutcome')\n",
    "plt.xlabel('Poutcome')\n",
    "plt.ylabel('Frequency of Purchase')\n",
    "plt.savefig('pur_fre_pout_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poutcome seems to be a good predictor of the outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dummy variables**  \n",
    "That is variables with only two values, zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars=['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\n",
    "for var in cat_vars:\n",
    "    cat_list='var'+'_'+var\n",
    "    cat_list = pd.get_dummies(data[var], prefix=var)\n",
    "    data1=data.join(cat_list)\n",
    "    data=data1\n",
    "    \n",
    "    \n",
    "cat_vars=['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\n",
    "data_vars=data.columns.values.tolist()\n",
    "to_keep=[i for i in data_vars if i not in cat_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final data columns will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final=data[to_keep]\n",
    "data_final.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Over-sampling using SMOTE**  \n",
    "With our training data created, I’ll up-sample the no-subscription using the SMOTE algorithm(Synthetic Minority Oversampling Technique). At a high level, SMOTE:\n",
    "1. Works by creating synthetic samples from the minor class (no-subscription) instead of creating copies.\n",
    "2. Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_final.loc[:, data_final.columns != 'y']\n",
    "y = data_final.loc[:, data_final.columns == 'y']\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "columns = X_train.columns\n",
    "os_data_X,os_data_y=os.fit_resample(X_train, y_train)\n",
    "os_data_X = pd.DataFrame(data=os_data_X,columns=columns )\n",
    "os_data_y= pd.DataFrame(data=os_data_y,columns=['y'])\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_data_X))\n",
    "print(\"Number of no subscription in oversampled data\",len(os_data_y[os_data_y['y']==0]))\n",
    "print(\"Number of subscription\",len(os_data_y[os_data_y['y']==1]))\n",
    "print(\"Proportion of no subscription data in oversampled data is \",len(os_data_y[os_data_y['y']==0])/len(os_data_X))\n",
    "print(\"Proportion of subscription data in oversampled data is \",len(os_data_y[os_data_y['y']==1])/len(os_data_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a perfect balanced data! You may have noticed that I over-sampled only on the training data, because by oversampling only on the training data, none of the information in the test data is being used to create synthetic observations, therefore, no information will bleed from test data into the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recursive Feature Elimination**  \n",
    "Recursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_vars=data_final.columns.values.tolist()\n",
    "y=['y']\n",
    "X=[i for i in data_final_vars if i not in y]\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFE(logreg, 20)\n",
    "rfe = rfe.fit(os_data_X, os_data_y.values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFE has helped us select the following features: “euribor3m”, “job_blue-collar”, “job_housemaid”, “marital_unknown”, “education_illiterate”, “default_no”, “default_unknown”, “contact_cellular”, “contact_telephone”, “month_apr”, “month_aug”, “month_dec”, “month_jul”, “month_jun”, “month_mar”, “month_may”, “month_nov”, “month_oct”, “poutcome_failure”, “poutcome_success”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['euribor3m', 'job_blue-collar', 'job_housemaid', 'marital_unknown', 'education_illiterate', 'default_no', 'default_unknown', \n",
    "      'contact_cellular', 'contact_telephone', 'month_apr', 'month_aug', 'month_dec', 'month_jul', 'month_jun', 'month_mar', \n",
    "      'month_may', 'month_nov', 'month_oct', \"poutcome_failure\", \"poutcome_success\"] \n",
    "X=os_data_X[cols]\n",
    "y=os_data_y['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing the model**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-values for most of the variables are smaller than 0.05, except two variables, therefore, we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['euribor3m', 'job_blue-collar', 'job_housemaid', 'default_no', 'default_unknown', \n",
    "      'contact_cellular', 'contact_telephone', 'month_apr', 'month_aug', 'month_dec', 'month_jul', 'month_jun', 'month_mar', \n",
    "      'month_may', 'month_nov', 'month_oct', \"poutcome_failure\", \"poutcome_success\"]\n",
    "X=os_data_X[cols]\n",
    "y=os_data_y['y']\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Model Fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is telling us that we have 7019+6341 correct predictions and 1334+647 incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute precision, recall, F-measure and support**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    "- The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "- The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "- The F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.  \n",
    "\n",
    "The support is the number of occurrences of each class in y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** Of the entire test set, 87% of the promoted term deposit were the term deposit that the customers liked. Of the entire test set, 87% of the customer’s preferred term deposits that were promoted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC Curve**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
