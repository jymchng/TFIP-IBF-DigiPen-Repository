{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Identification and Removal\n",
    "\n",
    "When modeling, it is important to clean the data sample to ensure that the observations best represent the problem. Sometimes a dataset can contain extreme values that are outside the range of what is expected and unlike the other data. These are called outliers and often machine learning modeling and model skill in general can be improved by understanding and even removing these outlier values.\n",
    "\n",
    "In this exercise, you will discover outliers and how to identify and remove them from your machine learning dataset.\n",
    "\n",
    "After completing this exercise, you will know:\n",
    "- That an outlier is an unlikely observation in a dataset and may have one of many causes.\n",
    "- How to use simple univariate statistics like standard deviation and interquartile range to identify and remove outliers from a data sample.\n",
    "- How to use an outlier detection model to identify and remove rows from a training dataset in order to lift predictive modeling performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Exercise Overview\n",
    "\n",
    "This exercise is divided into five parts; they are:\n",
    "1. What are Outliers?\n",
    "2. Test Dataset\n",
    "3. Standard Deviation Method\n",
    "4. Interquartile Range Method\n",
    "5. Automatic Outlier Detection\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What are Outliers?\n",
    "\n",
    "An outlier is an observation that is unlike the other observations. They are rare, distinct, or do not fit in some way. We will generally define outliers as samples that are exceptionally far from the mainstream of the data.\n",
    "\n",
    "Outliers can have many causes, such as:\n",
    "- Measurement or input error.\n",
    "- Data corruption.\n",
    "- True outlier observation.\n",
    "\n",
    "There is no precise way to defne and identify outliers in general because of the specifics of each dataset. Instead, you, or a domain expert, must interpret the raw observations and decide whether a value is an outlier or not.\n",
    "\n",
    "Even with a thorough understanding of the data, outliers can be hard to define. Great care should be taken not to hastily remove or change values, especially if the sample size is small.\n",
    "\n",
    "Nevertheless, we can use statistical methods to identify observations that appear to be rare or unlikely given the available data.\n",
    "\n",
    "This does not mean that the values identified are outliers and should be removed. But, the tools described in this exercise can be helpful in shedding light on rare events that may require a second look. A good tip is to consider plotting the identified outlier values, perhaps in the context of non-outlier values to see if there are any systematic relationship or pattern to the outliers. If there is, perhaps they are not outliers and can be explained, or perhaps the outliers themselves can be identified more systematically.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Test Dataset\n",
    "\n",
    "Before we look at outlier identification methods, let's define a dataset we can use to test the methods.\n",
    "\n",
    "Numbers drawn from a Gaussian distribution will have outliers. That is, by virtue of the distribution itself, there will be a few values that will be a long way from the mean, rare values that we can identify as outliers.\n",
    "\n",
    "- Using `randn()` from the `numpy.random` routine, generate a population 10,000 random numbers drawn from a Gaussian distribution with a mean of 50 and a standard deviation of 5. \n",
    "\n",
    "  Note: The `randn()` function generates a random Gaussian values with a mean of $0$ and a standard deviation of $1$. To get the preferred range, multiply the results by the standard deviation and add the mean to shift the values. You are to use a `seed(1)` before generating the random number (or rather pseudo-random) to ensure the same sample of numbers is generated each time the code is executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T05:58:11.423143Z",
     "start_time": "2021-01-15T05:58:11.419153Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T05:54:53.044209Z",
     "start_time": "2021-01-15T05:54:53.041207Z"
    }
   },
   "outputs": [],
   "source": [
    "oneThousand = 5 * np.random.randn(10000) + 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Determine the mean and standard deviation of the data you generated. You should expect your answer your values to be very close to 50 and 5 respectively. (Answer: mean=50.049 stdv=4.994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T05:55:21.061408Z",
     "start_time": "2021-01-15T05:55:21.054428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.04886328349552 4.993929218440242\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(oneThousand.mean(), oneThousand.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Standard Deviation Method\n",
    "\n",
    "If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers. The Gaussian distribution has the property that the standard deviation from the mean can be used to reliably summarize the percentage of values in the sample.\n",
    "\n",
    "In statistics, within one standard deviation of the mean will cover 68 percent of the data. We can cover more of the data sample if we expand therange as follows:\n",
    "- 1 Standard Deviation from the Mean: 68 percent.\n",
    "- 2 Standard Deviations from the Mean: 95 percent.\n",
    "- 3 Standard Deviations from the Mean: 99.7 percent.\n",
    "\n",
    "This is known as the [empirical rule](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule). So, if the mean is 50 and the standard deviation is 5, as in the test dataset above, then all data in the sample between 45 and 55 will account for about 68 percent of the data sample.\n",
    "\n",
    "A value that falls outside of 3 standard deviations is part of the distribution, but it is an unlikely or rare event at approximately 1 in 370 samples. Three standard deviations from the mean is a common cut-off in practice for identifying outliers in a Gaussian or Gaussian-like distribution. For smaller samples of data, perhaps a value of 2 standard deviations (95 percent) can be used, and for larger samples, perhaps a value of 4 standard deviations (99.9 percent) can be used.\n",
    "\n",
    "Given the mean $\\mu$ and standard deviation $\\sigma$, a simple way to identify outliers is to compute a *z-score* for every $x_i$. *Z-score* is defined as the number of standard deviations away $x_i$ is from the mean. Data values that have a z-score sigma greater than a threshold, for example, of three, are declared to be outliers.\n",
    "\n",
    "Let's make this concrete with a worked example. Sometimes, the data is standardized first (e.g. to a Z-score with zero mean and unit variance) so that the outlier detection can be performed using standard Z-score cut-off values. This is a convenience and is not required in general, and we will perform the calculations in the original scale of the data here to make things clear. We can calculate the mean and standard deviation of a given sample, then calculate the cut-off for identifying outliers as more than 3 standard deviations from the mean.\n",
    "```python\n",
    "'''Example of estimating the lower and upper bounds of the data.'''\n",
    "...\n",
    "# calculate summary statistics\n",
    "data_mean, data_std = mean(data), std(data)\n",
    "# define outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "```\n",
    "\n",
    "We can then identify outliers as those examples that fall outside of the defined lower and upper limits.\n",
    "```python\n",
    "'''Example of identifying outliers using the limits on the data.'''\n",
    "...\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "```\n",
    "\n",
    "Alternately, we can filter out those values from the sample that are not within the defined limits.\n",
    "```python\n",
    "'''Example of removing outliers from the data.'''\n",
    "...\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x > lower and x < upper]\n",
    "```\n",
    "\n",
    "- Using the methods described here, identify the number of outliers as compared to non-outlier observations from the data you have generated previously.\n",
    "  \n",
    "  Expected output\n",
    "  ```\n",
    "  Identified outliers: 29\n",
    "  Non-outlier observations: 9971\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T05:58:56.401708Z",
     "start_time": "2021-01-15T05:58:56.390740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified outliers: 31\n",
      "Non-outlier observations: 9969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = 5 * np.random.randn(10000) + 50\n",
    "\n",
    "# calculate summary statistics\n",
    "data_mean, data_std = np.mean(data), np.std(data)\n",
    "# define outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "outliers_removed = [x for x in data if x > lower and x < upper]\n",
    "\n",
    "print(f\"\"\"Identified outliers: {len(outliers)}\n",
    "Non-outlier observations: {len(outliers_removed)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only talked about univariate data with a Gaussian distribution, e.g. a single variable. You can use the same approach if you have multivariate data, e.g. data with multiple variables, each with a different Gaussian distribution. You can imagine bounds in two dimensions that would define an ellipse if you have two variables. Observations that fall outside of the ellipse would be considered outliers. In three dimensions, this would be an ellipsoid, and so on into higher dimensions. Alternately, if you knew more about the domain, perhaps an outlier may be identified by exceeding the limits on one or a subset of the data dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Interquartile Range Method\n",
    "\n",
    "Not all data is normal or normal enough to treat it as being drawn from a Gaussian distribution. A good statistic for summarizing a non-Gaussian distribution sample of data is the Interquartile Range, or IQR for short. The IQR is calculated as the difference between the 75th and the 25th percentiles of the data and defines the box in a box and whisker plot. Remember that percentiles can be calculated by sorting the observations and selecting values at specific indices. The 50th percentile is the middle value, or the average of the two middle values for an even number of examples. If we had 10,000 samples, then the 50th percentile would be the average of the 5000th and 5001st values.\n",
    "\n",
    "We refer to the percentiles as quartiles (quart meaning 4) because the data is divided into four groups via the 25th, 50th and 75th values. The IQR defines the middle 50 percent of the data, or the body of the data.\n",
    "\n",
    "The IQR can be used to identify outliers by defining limits on the sample values that are a factor k of the IQR below the 25th percentile or above the 75th percentile. The common value for the factor $k$ is the value 1.5. A factor $k$ of 3 or more can be used to identify values that are extreme outliers or far outs when described in the context of box and whisker plots. On a box and whisker plot, these limits are drawn as fences on the whiskers (or the lines) that are drawn from the box. Values that fall outside of these values are drawn as dots. We can calculate the percentiles of a dataset using the `percentile()` NumPy function that takes the dataset and specification of the desired percentile. The IQR can then be calculated as the difference between the 75th and 25th percentiles.\n",
    "\n",
    "```python\n",
    "'''Example of calculating quartiles on the data.'''\n",
    "...\n",
    "# calculate interquartile range\n",
    "q25, q75 = percentile(data, 25), percentile(data, 75)\n",
    "iqr = q75 - q25\n",
    "Listing 6.8: Example of calculating quartiles on the data.\n",
    "```\n",
    "\n",
    "We can then calculate the cutoff for outliers as 1.5 times the IQR and subtract this cut-off from the 25th percentile and add it to the 75th percentile to give the actual limits on the data.\n",
    "\n",
    "```python\n",
    "'''Example of calculating lower and upper bounds using the IQR.'''\n",
    "...\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 1.5\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "```\n",
    "\n",
    "We can then use these limits to identify the outlier values.\n",
    "```python\n",
    "'''Example of identifying outliers using the limits on the data.'''\n",
    "...\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "```\n",
    "\n",
    "We can also use the limits to filter out the outliers from the dataset.\n",
    "\n",
    "```python\n",
    "'''Example of removing outliers from the data.'''\n",
    "...\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x > lower and x < upper]\n",
    "```\n",
    "\n",
    "- Using the methods described here, identify the number of outliers as compared to non-outlier observations from the data you have generated previously. Your code print 25th and 75th percentiles and the calculated IQR.\n",
    "  \n",
    "  Expected output\n",
    "  ```\n",
    "  Percentiles: 25th=46.685, 75th=53.359, IQR=6.674\n",
    "  Identified outliers: 81\n",
    "  Non-outlier observations: 9919\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T06:08:49.038523Z",
     "start_time": "2021-01-15T06:08:49.026555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output\n",
      "\n",
      "Percentiles: 25th=46.56730941273662, 75th=53.21470000746439, IQR=6.647390594727767\n",
      "Identified outliers: 75\n",
      "Non-outlier observations: 9925\n"
     ]
    }
   ],
   "source": [
    "q25, q75 = np.percentile(data, 25), np.percentile(data, 75)\n",
    "iqr = q75 - q25\n",
    "\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 1.5\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x > lower and x < upper]\n",
    "\n",
    "print(f\"\"\"Expected output\n",
    "\n",
    "Percentiles: 25th={q25}, 75th={q75}, IQR={iqr}\n",
    "Identified outliers: {len(outliers)}\n",
    "Non-outlier observations: {len(outliers_removed)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach can be used for multivariate data by calculating the limits on each variable in the dataset in turn, and taking outliers as observations that fall outside of the rectangle or hyper-rectangle.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Automatic Outlier Detection\n",
    "\n",
    "In machine learning, an approach to tackling the problem of outlier detection is one-class classification. A simple approach to identifying outliers is to locate those examples that are far from the other examples in the multi-dimensional feature space. This can work well for feature spaces with low dimensionality (few features), although it can become less reliable as the number of features is increased, referred to as the curse of dimensionality. The local outlier factor, or `LOF` for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers. The scikit-learn library provides an implementation of this approach in the `LocalOutlierFactor` class.\n",
    "\n",
    "We can demonstrate the `LocalOutlierFactor` method on a predictive modeling dataset. \n",
    "\n",
    "We will use the Boston housing regression problem that has 13 inputs and one numerical target and requires learning the relationship between suburb characteristics and house prices.\n",
    "\n",
    "- Using Pandas, load the dataset and peek at the top 5 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T06:09:31.514172Z",
     "start_time": "2021-01-15T06:09:30.381117Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T06:09:56.392419Z",
     "start_time": "2021-01-15T06:09:56.371474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
       "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
       "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
       "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
       "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
       "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
       "\n",
       "       11    12    13  \n",
       "0  396.90  4.98  24.0  \n",
       "1  396.90  9.14  21.6  \n",
       "2  392.83  4.03  34.7  \n",
       "3  394.63  2.94  33.4  \n",
       "4  396.90  5.33  36.2  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = pd.read_csv('housing.csv', header = None)\n",
    "\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking in the dataset, you should see that all variables are numeric.\n",
    "\n",
    "- Next, retrieve the data as a NumPy array using `to_numpy()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T06:10:22.187087Z",
     "start_time": "2021-01-15T06:10:22.184095Z"
    }
   },
   "outputs": [],
   "source": [
    "housing = housing.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, separate it into input `X` and output `y` variables. They should have a shape of (506, 13) and (506,) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T06:17:29.964106Z",
     "start_time": "2021-01-15T06:17:29.960117Z"
    }
   },
   "outputs": [],
   "source": [
    "Y = housing[:,-1]\n",
    "X = housing[:,:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `train_test_split` from the `sklearn.model_selection` class, split your data into train and test datasets. Use a test size of 0.33 and a random state of 1. You should expect the output of shapes for `X_train.shape, X_test.shape, y_train.shape, y_test.shape` to be `(339, 13) (167, 13) (339,) (167,)` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T06:17:31.091244Z",
     "start_time": "2021-01-15T06:17:31.086259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (167, 13) (339,) (167,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.33, random_state = 1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will later learn that this is a regression predictive modeling problem, meaning that we will be predicting a numeric value. All input variables are also numeric. In this case, we will fit a linear regression algorithm and evaluate model performance by training the model on the training dataset and making a prediction on the test data and evaluate the predictions using the mean absolute error (MAE).\n",
    "\n",
    "An example of evaluating a linear regression model on the dataset is listed below.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "...\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)\n",
    "```\n",
    "\n",
    "- Using the example above, evaluate the MAE of your data assuming a linear regression model. (Answer: 3.417)\n",
    "  \n",
    "  Note: Your specific results may vary given the stochastic nature of the learning algorithm, the evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T06:17:32.162200Z",
     "start_time": "2021-01-15T06:17:32.155219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try removing outliers from the training dataset. The expectation is that the outliers are causing the linear regression model to learn a bias or skewed understanding of the problem, and that removing these outliers from the training set will allow a more effective model to be learned. We can achieve this by defining the `LocalOutlierFactor` model and using it to make a prediction on the training dataset, marking each row in the training dataset as normal (1) or an outlier (-1). We will use the default hyperparameters for the outlier detection model, although it is a good idea to tune the configuration to the specifics of your dataset.\n",
    "\n",
    "```python\n",
    "'''Example of identifying outliers automatically.'''\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "...\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "```\n",
    "\n",
    "We can then use these predictions to remove all outliers from the training dataset.\n",
    "\n",
    "```python\n",
    "'''Example of removing identified outliers from the dataset.'''\n",
    "...\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "```\n",
    "\n",
    "- With the outlier removed, re-evaluate your model on the regression dataset from the training dataset. (Answer: 3.356)\n",
    "\n",
    "  Note: Your specific results may vary given the stochastic nature of the learning algorithm, the evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T06:16:55.928514Z",
     "start_time": "2021-01-15T06:16:55.752076Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Example of identifying outliers automatically.'''\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "'''Example of removing identified outliers from the dataset.'''\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we can see that the number of examples in the training dataset has been reduced from 339 to 305, meaning 34 rows containing outliers were identified and deleted. We can also see a reduction in MAE from about 3.417 by a model fit on the entire training dataset, to about 3.356 on a model fit on the dataset with outliers removed.\n",
    "\n",
    "The Scikit-Learn library provides other outlier detection algorithms that can be used in the same way such as the Isolation Forest algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Further Reading\n",
    "\n",
    "- Outlier on Wikipedia\n",
    "  https://en.wikipedia.org/wiki/Outlier\n",
    "  \n",
    "- Anomaly detection on Wikipedia.\n",
    "  https://en.wikipedia.org/wiki/Anomaly_detection\n",
    "\n",
    "- 68-95-99.7 rule on Wikipedia.\n",
    "  https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule\n",
    "\n",
    "- Interquartile range.\n",
    "  https://en.wikipedia.org/wiki/Interquartile_range\n",
    "\n",
    "- Box plot on Wikipedia.\n",
    "  https://en.wikipedia.org/wiki/Box_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
