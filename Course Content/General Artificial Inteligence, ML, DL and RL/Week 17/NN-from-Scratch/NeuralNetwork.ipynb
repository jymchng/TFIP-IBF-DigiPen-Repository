{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optimizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2da54a601207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnparry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optimizers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import os\n",
    "from optimizers import *\n",
    "\n",
    "def print_shape(nparry):\n",
    "    print(\"{}\".format(nparry.shape))\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    \"\"\" Feedforward neural network with a single hidden layer\n",
    "        Params:\n",
    "        n_output: int: number of output units, equal to num class labels\n",
    "        n_features: int: number of features in the input dataset\n",
    "        n_hidden: int: (default 30): num hidden units\n",
    "        l2: float(default: 0.0) - lambda value for L2 regularization\n",
    "        epochs: int (default = 500) - passes over training set\n",
    "        learning_rate: float (default: 0.001) - learning reate\n",
    "        momentum_const: float (default: 0.0) - momentum constant - multiplied with gradient of previous pass through set\n",
    "        decay_rate: float (default 0.0) - shrinks learning rate after each epoch\n",
    "        minibatch_size: int (default: 1) - divides training data into batches for efficiency\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_output, n_features, n_hidden=30, l2=0.0, epochs=500,\n",
    "                 learning_rate=0.001, momentum_const=0.0, decay_rate=0.0,\n",
    "                 dropout=False, minibatch_size=1,\n",
    "                 optimizer = 'Gradient Descent', activation = 'relu',\n",
    "                 nesterov = False, check_gradients = False, early_stop = None, metrics = ['Accuracy']):\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self.initialize_weights()\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum_const = momentum_const\n",
    "        self.decay_rate = decay_rate\n",
    "        self.dropout = dropout\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.nesterov = nesterov\n",
    "        self.check_gradients = check_gradients\n",
    "        supported_optimizers = ['Gradient Descent', 'Momentum', 'Nesterov', 'Adam', 'Adagrad', 'Adadelta', 'RMSProp']\n",
    "        if optimizer not in supported_optimizers:\n",
    "            print(\"Error: unsupported optimizer requested.\")\n",
    "            print(\"Available optimizers: {}\".format(supported_optimizers))\n",
    "            exit()\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        supported_activations = ['relu', 'tanh', 'sigmoid', 'maxout', 'elu']\n",
    "        if activation not in supported_activations:\n",
    "            print(\"Error: unsupported activation requested.\")\n",
    "            print(\"Available activations: {}\".format(supported_activations))\n",
    "        else:\n",
    "            self.activation = activation\n",
    "        self.early_stop = early_stop\n",
    "        SUPPORTED_METRICS = ['Accuracy', 'Precision', 'Recall', 'AUC']\n",
    "        for elem in metrics:\n",
    "            assert elem in SUPPORTED_METRICS\n",
    "        self.metrics = metrics\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        init weights with random nums uniformly with small values, dividing by fanin for xavier\n",
    "        \"\"\"\n",
    "        w1 = np.random.uniform(-1.0, 1.0, size = self.n_hidden * (self.n_features + 1)).reshape(self.n_hidden, (self.n_features + 1))/(self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0, size=self.n_output*(self.n_hidden+1)).reshape(self.n_output, self.n_hidden+ 1)/(self.n_hidden + 1)\n",
    "        return w1, w2\n",
    "\n",
    "    def encode_labels(self, y, num_labels):\n",
    "        \"\"\" Encode labels into a one-hot representation\n",
    "            Params:\n",
    "            y: array of num_samples, contains the target class labels for each training example.\n",
    "            For example, y = [2, 1, 3, 3] -> 4 training samples, and the ith sample has label y[i]\n",
    "            k: number of output labels\n",
    "            returns: onehot, a matrix of labels by samples. For each column, the ith index will be\n",
    "            \"hot\", or 1, to represent that index being the label.\n",
    "        \"\"\"\n",
    "        onehot = np.zeros((num_labels, y.shape[0]))\n",
    "        for i in range(y.shape[0]):\n",
    "            onehot[y[i], i] = 1.0\n",
    "        return onehot\n",
    "\n",
    "    def softmax(self, v):\n",
    "        \"\"\"Calculates the softmax function that outputs a vector of values that sum to one.\n",
    "            We take max(softmax(v)) to be the predicted label. The output of the softmax function\n",
    "            is also used to calculate the cross-entropy loss\n",
    "        \"\"\"\n",
    "        logC = -np.max(v)\n",
    "        return np.exp(v + logC)/np.sum(np.exp(v + logC), axis = 0)\n",
    "\n",
    "    def tanh(self, z, deriv=False):\n",
    "        \"\"\" Compute the tanh function or its derivative.\n",
    "        \"\"\"\n",
    "        return np.tanh(z) if not deriv else 1 - np.square(np.tanh(z))\n",
    "\n",
    "    def relu(self, z, deriv = False):\n",
    "        if not deriv:\n",
    "            relud = z\n",
    "            relud[relud < 0] = 0\n",
    "            return relud\n",
    "        deriv = z\n",
    "        deriv[deriv <= 0] = 0\n",
    "        deriv[deriv > 0] = 1\n",
    "        return deriv\n",
    "\n",
    "\n",
    "    def add_bias_unit(self, X, column=True):\n",
    "        \"\"\"Adds a bias unit to our inputs\"\"\"\n",
    "        if column:\n",
    "            bias_added = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            bias_added[:, 1:] = X\n",
    "        else:\n",
    "            bias_added = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            bias_added[1:, :] = X\n",
    "\n",
    "        return bias_added\n",
    "\n",
    "    def compute_dropout(self, activations, dropout_prob = 0.5):\n",
    "        \"\"\"Sets half of the activations to zero\n",
    "        Params: activations - numpy array\n",
    "        Return: activations, which half set to zero\n",
    "        \"\"\"\n",
    "        # handle error\n",
    "        if dropout_prob < 0 or dropout_prob > 1:\n",
    "            dropout_prob = 0.5\n",
    "        # scale the activations (see http://cs231n.github.io/neural-networks-2/)\n",
    "        activations/=dropout_prob    \n",
    "        mult = np.random.binomial(1, dropout_prob, size = activations.shape)\n",
    "        activations*=mult\n",
    "        return activations\n",
    "\n",
    "    def forward(self, X, w1, w2, do_dropout = True):\n",
    "        \"\"\" Compute feedforward step\n",
    "            Params:\n",
    "            X: matrix of num_samples by num_features, input layer with samples and features\n",
    "            w1: matrix of weights from input layer to hidden layer. Dimensionality of num_hidden_units by num_features + 1 (bias)\n",
    "            w2: matrix of weights from hidden layer to output layer. Dimensionality of num_output_units (equal to num class labels) by num_hidden units + 1 (bias)\n",
    "            dropout: If true, randomly set half of the activations to zero to prevent overfitting.\n",
    "        \"\"\"\n",
    "        #the activation of the input layer is simply the input matrix plus bias unit, added for each sample.\n",
    "        a1 = self.add_bias_unit(X)\n",
    "        if self.dropout and do_dropout: a1 = self.compute_dropout(a1)\n",
    "        #the input of the hidden layer is obtained by applying our weights to our inputs. We essentially take a linear combination of our inputs\n",
    "        z2 = w1.dot(a1.T)\n",
    "        #applies the tanh function to obtain the input mapped to a distrubution of values between -1 and 1\n",
    "        a2 = self.tanh(z2)\n",
    "        #add a bias unit to activation of the hidden layer.\n",
    "        a2 = self.add_bias_unit(a2, column=False)\n",
    "        if self.dropout and do_dropout: a2 = self.compute_dropout(a2)\n",
    "        # compute input of output layer in exactly the same manner.\n",
    "        z3 = w2.dot(a2)\n",
    "        # the activation of our output layer is just the softmax function.\n",
    "        a3 = self.softmax(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\" Compute the cost function.\n",
    "            Params:\n",
    "            y_enc: array of num_labels x num_samples. class labels one-hot encoded\n",
    "            output: matrix of output_units x samples - activation of output layer from feedforward\n",
    "            w1: weight matrix of input to hidden layer\n",
    "            w2: weight matrix of hidden to output layer\n",
    "            \"\"\"\n",
    "        cost = - np.sum(y_enc*np.log(output))\n",
    "        # add the L2 regularization by taking the L2-norm of the weights and multiplying it with our constant.\n",
    "        l2_term = (self.l2/2.0) * (np.sum(np.square(w1[:, 1:])) + np.sum(np.square(w2[:, 1:])))\n",
    "        cost = cost + l2_term\n",
    "        return cost/y_enc.shape[1]\n",
    "\n",
    "    def backprop(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Computes the gradient using backpropagation\n",
    "            Params:\n",
    "            a1: array of n_samples by features+1 - activation of input layer (just input plus bias)\n",
    "            a2: activation of hidden layer\n",
    "            a3: activation of output layer\n",
    "            z2: input of hidden layer\n",
    "            y_enc: onehot encoded class labels\n",
    "            w1: weight matrix of input layer to hidden layer\n",
    "            w2: weight matrix of hidden to output layer\n",
    "            returns: grad1, grad2: gradient of weight matrix w1, gradient of weight matrix w2\n",
    "        \"\"\"\n",
    "        #backpropagate our error\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self.add_bias_unit(z2, column=False)\n",
    "        sigma2 = w2.T.dot(sigma3) * self.tanh(z2, deriv=True)\n",
    "        #get rid of the bias row\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "         # add the regularization term\n",
    "        grad1[:, 1:]+= (w1[:, 1:]*self.l2) # derivative of .5*l2*w1^2\n",
    "        grad2[:, 1:]+= (w2[:, 1:]*self.l2) # derivative of .5*l2*w2^2\n",
    "        return grad1, grad2\n",
    "\n",
    "    def accuracy(self, X_train, y_train):\n",
    "        \"\"\"Calculate the training accuracy. Requires passing through the entire dataset.\"\"\"\n",
    "        y_train_pred = self.predict(X_train)\n",
    "        diffs = y_train_pred - y_train\n",
    "        count = 0.\n",
    "        for i in range(y_train.shape[0]):\n",
    "            if diffs[i] != 0:\n",
    "                count+=1\n",
    "        return 100 - count*100/y_train.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X, dropout = False):\n",
    "        \"\"\"Generate a set of predicted labels for the input dataset\"\"\"\n",
    "        a1, z2, a2, z3, a3 = self.forward(X, self.w1, self.w2, do_dropout = False)\n",
    "        #z3 is of dimension output units x num_samples. each row is an array representing the likelihood that the sample belongs to the class label given by the index...\n",
    "        #ex: first row of z3 = [0.98, 0.78, 0.36]. This means our network has 3 output units = 3 class labels. And this instance most likely belongs to the class given by the label 0.\n",
    "        y_pred = np.argmax(a3, axis = 0)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=True):\n",
    "        \"\"\" Learn weights from training data\n",
    "            Params:\n",
    "            X: matrix of samples x features. Input layer\n",
    "            y: target class labels of the training instances (ex: y = [1, 3, 4, 4, 3])\n",
    "            print_progress: True if you want to see the loss and training accuracy, but it is expensive.\n",
    "        \"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self.encode_labels(y, self.n_output)\n",
    "        # PREVIOUS GRADIENTS\n",
    "        prev_grad_w1 = np.zeros(self.w1.shape)\n",
    "        prev_grad_w2 = np.zeros(self.w2.shape)\n",
    "        print(\"fitting\")\n",
    "        costs = []\n",
    "        grad_1_li, grad_2_li = [], [] # used to keep list of gradients which can be used to measure and differentiate between learning speed of input -> hidden and hidden -> output layer weights\n",
    "\n",
    "        #pass through the dataset\n",
    "        for i in range(self.epochs):\n",
    "            previous_accuracies = []\n",
    "            self.learning_rate /= (1 + self.decay_rate*i)\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatch_size)\n",
    "            grads_w1, grads_w2 = [], [] # needed if we want to remember averages of gradients across time\n",
    "            for idx in mini:\n",
    "                #feed feedforward\n",
    "                a1, z2, a2, z3, a3= self.forward(X_data[idx], self.w1, self.w2)\n",
    "                cost = self.get_cost(y_enc=y_enc[:, idx], output=a3, w1=self.w1, w2=self.w2)\n",
    "                costs.append(cost)\n",
    "\n",
    "                #compute gradient via backpropagation\n",
    "\n",
    "                grad1, grad2 = self.backprop(a1=a1, a2=a2, a3=a3, z2=z2, y_enc=y_enc[:, idx], w1=self.w1, w2=self.w2)\n",
    "                grad_1_li.append(grad1)\n",
    "                grad_2_li.append(grad2)\n",
    "\n",
    "                if self.check_gradients:\n",
    "                    eps = 10e-4\n",
    "                    # compute numerical gradient\n",
    "                    w1_check, w2_check = self.w1, self.w2\n",
    "                    g1_check, g2_check = grad1, grad2\n",
    "                    w1_check = w1_check.reshape((w1_check.shape[0] * w1_check.shape[1]))\n",
    "                    w2_check = w2_check.reshape((w2_check.shape[0] * w2_check.shape[1]))\n",
    "                    g1_check = g1_check.reshape((g1_check.shape[0] * g1_check.shape[1]))\n",
    "                    g2_check = g2_check.reshape((g2_check.shape[0] * g2_check.shape[1]))\n",
    "                    for check in [w1_check, w2_check, g1_check, g2_check]:\n",
    "                        print_shape(check)\n",
    "                    for i in range(w1_check.shape[0]):\n",
    "                        E = np.zeros(w1_check.shape[0])\n",
    "                        E[i] = 1\n",
    "                        w1_check_2 = w1_check + (eps * E)\n",
    "                        w1_check_3 = w1_check - (eps * E)\n",
    "                        for j in range(w1_check.shape[0]):\n",
    "                            if j != i:\n",
    "                                assert w1_check_2[j] == w1_check[j], \"Houston we have a problem\"\n",
    "                                assert w1_check_3[j] == w1_check[j], \"Houston we have a problem\"\n",
    "                        J1 = self.get_cost(y_enc=y_enc[:, idx], output=a3, w1=w1_check_2.reshape((self.w1.shape[0], self.w1.shape[1])), w2=self.w2)\n",
    "                        J2 = self.get_cost(y_enc=y_enc[:, idx], output=a3, w1=w1_check_3.reshape((self.w1.shape[0], self.w1.shape[1])), w2=self.w2)\n",
    "                        numerical_gradient = (J1 - J2) / (2 * eps)\n",
    "                        if i % 100 == 0:\n",
    "                            print(\"numerical gradient is {} and actual gradient is {}\".format(numerical_gradient, g1_check[i]))\n",
    "\n",
    "                # update parameters, multiplying by learning rate + momentum constants\n",
    "                # w1_update, w2_update = self.momentum_optimizer(self.learning_rate, grad1, grad2)\n",
    "                w1_update, w2_update = self.learning_rate*grad1, self.learning_rate*grad2\n",
    "                # OPTIMIZERS TEST - remove this later\n",
    "                # w1_update, w2_update = vanilla_gd([self.learning_rate], grad1, grad2)\n",
    "                # self.w1 += -(w1_update)\n",
    "                # self.w2 += -(w2_update)\n",
    "                # continue\n",
    "                if self.nesterov:\n",
    "                    # v_prev = v # back this up\n",
    "                    # v = mu * v - learning_rate * dx # velocity update stays the same\n",
    "                    # x += -mu * v_prev + (1 + mu) * v # position update changes form\n",
    "                    # psuedocode from http://cs231n.github.io/neural-networks-3/#sgd\n",
    "                    v1 = self.momentum_const * prev_grad_w1 - w1_update\n",
    "                    v2 = self.momentum_const * prev_grad_w2 - w2_update\n",
    "                    self.w1 += -self.momentum_const * prev_grad_w1 + (1 + self.momentum_const) * v1\n",
    "                    self.w2 += -self.momentum_const * prev_grad_w2 + (1 + self.momentum_const) * v2\n",
    "                else:\n",
    "                    # gradient update: w += -alpha * gradient.\n",
    "                    # use momentum - add in previous gradient mutliplied by a momentum hyperparameter.\n",
    "                    self.w1 += -(w1_update + (self.momentum_const*prev_grad_w1))\n",
    "                    self.w2 += -(w2_update + (self.momentum_const*prev_grad_w2))\n",
    "                # save previous gradients for momentum\n",
    "                # grads_w1.append(w1_update)\n",
    "                # grads_w2.append(w2_update)\n",
    "                # prev_grad_w1 = np.mean(grads_w1)\n",
    "                # prev_grad_w2 = np.mean(grads_w2)\n",
    "                # alternate way that just remembers the previous gradient\n",
    "                prev_grad_w1, prev_grad_w2 = w1_update, w2_update\n",
    "\n",
    "            if print_progress and (i+1) % 1 == 0:\n",
    "                print(\"Epoch: {}\".format(i + 1))\n",
    "                print(\"Loss: {}\".format(cost))\n",
    "                if self.check_gradients:\n",
    "                    print(\"Gradient Error: {}\".format(w1_grad_error))\n",
    "                grad_1_mag, grad_2_mag = np.linalg.norm(grad_1_li), np.linalg.norm(grad_2_li)\n",
    "                acc = self.accuracy(X, y)\n",
    "                previous_accuracies.append(acc)\n",
    "                if self.early_stop is not None and len(previous_accuracies) > 3:\n",
    "                    if abs(previous_accuracies[-1] - previous_accuracies[-2]) < self.early_stop and abs(previous_accuracies[-1] - previous_accuracies[-3]) < self.early_stop:\n",
    "                        print(\"Early stopping, accuracy has stayed roughly constant over last 100 iterations.\")\n",
    "                        break\n",
    "\n",
    "                print(\"Training Accuracy: {}\".format(acc))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
