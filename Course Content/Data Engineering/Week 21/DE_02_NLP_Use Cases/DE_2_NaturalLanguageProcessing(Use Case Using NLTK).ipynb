{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "The dataset which we are going to use is an open-source dataset available on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the dataset**   \n",
    "The dataset contains three columns. The size of the dataset is around 5.65mb. It has around 5000 rows in total.\n",
    "\n",
    "**Columns**  \n",
    "Label: ham, spam\n",
    "Text: a collection of text or emails\n",
    "Label_num: 0 for ham and 1 for spam\n",
    "\n",
    "**Task**  \n",
    "Our Task is to create a machine learning model that can accurately predict whether an email is a spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using ML and NLP\n",
    "\n",
    "**Importing necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np ## scientific computation\n",
    "import pandas as pd ## loading dataset file\n",
    "import matplotlib.pyplot as plt ## Visulization\n",
    "import nltk  ## Preprocessing our text\n",
    "from nltk.corpus import stopwords ## removing all the stop words\n",
    "from nltk.stem.porter import PorterStemmer ## stemming of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spam_ham_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)  ### Return the shape of data \n",
    "print(\"**************************************\")\n",
    "print(df.size)   ### Return the size of data \n",
    "print(\"**************************************\")\n",
    "print(df.isna().sum())  ### Returns the sum fo all na values\n",
    "print(\"**************************************\")\n",
    "print(df.info())  ### Give concise summary of a DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see that we don’t have any null values in our dataset. Also, one thing to notice is that only one column of our has numerical values so we can only visualize that column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let’s Visualize the Column label_num**  \n",
    "We can only visualize the count of both the categories in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_num\"].value_counts().plot(kind=\"bar\",figsize=(12,6))\n",
    "plt.xticks(np.arange(2), ('Non spam', 'spam'),rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in the plot, we can see that almost 3500 does not spam and around 1500 are spam messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning The Text  \n",
    "\n",
    "In any NLP problem, the most important step is to clean the text. cleaning text means removing all the punctuation, removing stopwords, performing stemming, lemmatization, and converting the text into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "corpus = []\n",
    "length = len(df)\n",
    "for i in range(0,length):\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\",\" \",df[\"text\"][i])\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    pe = PorterStemmer()\n",
    "    stopword = stopwords.words(\"english\")\n",
    "    text = [pe.stem(word) for word in text if not word in set(stopword)]\n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "\n",
    "- **Line 1:** We are importing re library, which is used to perform regex in python.  \n",
    "- **Line 2:** Define an empty corpus list, that can be used to store all the text after cleaning.  \n",
    "- **Line 3:** Initializing the var length with the length of the data frame.  \n",
    "- **Line 4:** Running a loop from 0 to the length of our data frame.  \n",
    "- **Line 5:** Removing all characters except the lower alphabet, bigger alphabets, and digits.  \n",
    "- **Line 6:** Converting the text to lower.  \n",
    "- **Line 7:** Splitting the text by spaces.  \n",
    "- **Line 8:** Creating an object of porter stemmer.  \n",
    "- **Line 9:** Initializing all the stopword in English dictionary to var stopword  \n",
    "- **Line 10:** Running a loop in the length of the sentence and then for each word in the sentence checking it in stopword and if it does not find in stopword then apply Stemming on to the text and add it to the list.  \n",
    "- **Line 11:** Just concatenating all the words to make a sentence  \n",
    "- **Line 12:** Appending the sentence to the corpus list  \n",
    "- **Line 13:** Printing the corpus list.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Cleaning Process the next step is to convert the list of the sentence(corpus) into vectors so that we can feed this data into our machine learning model. For converting the text into vectors we are going to use a bag of words which is going to convert the text into binary form.’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=35000)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "y = df['label_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Line 1:** We are importing the CountVectorizer from sklearn.\n",
    "- **Line 2:** Creating an object for the count vectorizer with max features as 35000, means we are only fetching the top 35000 columns.\n",
    "- **Line 3:** Using CV we are fitting are corpus and also transforming it into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Training\n",
    "Splitting data into train and validation sets using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model using MultinomialNaiveBayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model to the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model\n",
    "we are going to evaluate our model using the confusion matrix and accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(cm,score*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
